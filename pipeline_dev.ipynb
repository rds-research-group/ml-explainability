{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing, svm, metrics, tree, decomposition, svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Lasso, Ridge, Perceptron, SGDClassifier, OrthogonalMatchingPursuit\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import plot_confusion_matrix, accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid, KFold, StratifiedKFold\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "\n",
    "import yaml\n",
    "import pickle\n",
    "from statistics import mean\n",
    "import random\n",
    "\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changes to be made:\n",
    "# 1. Add subgroups into model trainer class\n",
    "# 2. \"Functionize\" model score saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_fcns = {\n",
    "    'DC': DummyClassifier(strategy=\"uniform\"),\n",
    "    'RF': RandomForestClassifier(n_estimators=50, n_jobs=-1, random_state=42),\n",
    "    'ET': ExtraTreesClassifier(n_estimators=10, n_jobs=-1, criterion='entropy'),\n",
    "    'AB': AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), algorithm=\"SAMME\", n_estimators=200),\n",
    "    'LR': LogisticRegression(random_state=42,solver='liblinear'),\n",
    "    'SVM': svm.SVC(kernel='linear', probability=True, random_state=0),\n",
    "    'GB': GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=10),\n",
    "    'NB': GaussianNB(),\n",
    "    'DT': DecisionTreeClassifier(random_state=42),\n",
    "    'SGD': SGDClassifier(loss=\"hinge\", penalty=\"l2\"),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=3),\n",
    "    'LRR': Ridge(alpha=5.0,fit_intercept=True,random_state=42)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, MetaData, Table, Column, Integer, String, insert\n",
    "\n",
    "POSTGRES_ADDRESS = 'cs003-ib0'\n",
    "POSTGRES_PORT = '5432'\n",
    "POSTGRES_USERNAME = 'isk273'\n",
    "POSTGRES_PASSWORD = 'andrewIan'\n",
    "POSTGRES_DBNAME = 'explainability_db'\n",
    "\n",
    "postgres_str = (\n",
    "    f'postgresql://{POSTGRES_USERNAME}:{POSTGRES_PASSWORD}@{POSTGRES_ADDRESS}:{POSTGRES_PORT}/{POSTGRES_DBNAME}'\n",
    ")\n",
    "\n",
    "# Create the connection\n",
    "engine = create_engine(postgres_str, echo = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        xid,\n",
    "        X_name,\n",
    "        X,\n",
    "        y,\n",
    "        clfs,\n",
    "        hyperparameters,\n",
    "        split_methods,\n",
    "        split_random_states        \n",
    "    ):\n",
    "        self.xid = xid\n",
    "        self.X_name = X_name\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.split_random_states = split_random_states\n",
    "        self.split_methods = split_methods\n",
    "        self.set_clfs(clfs)\n",
    "        self.set_hyperparemeters(hyperparameters)\n",
    "        self.results_df = pd.DataFrame(columns=('xid','datetime','X_name','subgroup','split_method', 'split_seed', 'split_n', 'clf','params','metric','score'))\n",
    "    \n",
    "    # Check for presets\n",
    "    def set_hyperparemeters(self, hyperparemeters):\n",
    "        if hyperparemeters == 'small':\n",
    "            self.hyperparemeters = small_grid\n",
    "        elif hyperparemeters == 'large':\n",
    "            self.hyperparemeters = large_grid\n",
    "        elif hyperparemeters == 'test':\n",
    "            self.hyperparemeters = test_grid\n",
    "        else:\n",
    "            self.hyperparemeters = hyperparemeters\n",
    "    \n",
    "    # Check for presets\n",
    "    def set_clfs(self, clfs):\n",
    "        if clfs == 'all':\n",
    "            self.clfs = clf_list_all\n",
    "        elif clfs =='test':\n",
    "            self.clfs = clf_list_test\n",
    "        else:\n",
    "            self.clfs = clfs\n",
    "            \n",
    "    \n",
    "            \n",
    "    def train_models(self):\n",
    "        \n",
    "        # Evaluator?\n",
    "        # Initialize the evaluator\n",
    "        evaluator = ModelEvaluator(metrics='test')\n",
    "        \n",
    "        models_to_run = list(self.clfs.keys())\n",
    "        grid = self.hyperparemeters\n",
    "        for index,clf in enumerate([self.clfs[x] for x in models_to_run]):\n",
    "            parameter_values = grid[models_to_run[index]]\n",
    "            #i = 0\n",
    "            for p in ParameterGrid(parameter_values):\n",
    "                #i+=1\n",
    "                #print(f\"Training {models_to_run[index]}: {i} / {len(ParameterGrid(parameter_values))}\")\n",
    "                try:\n",
    "                    clf.set_params(**p)\n",
    "                    for r in self.split_random_states:\n",
    "                        for split_method in self.split_methods:\n",
    "                            if split_method.find('@') < 0:\n",
    "                                raise ValueError('You must define a number of folds or percentage for the split functions.')\n",
    "                            else:\n",
    "                                split_function = split_method[0:split_method.find('@')]\n",
    "                                n = int(split_method[split_method.find('@')+1:])\n",
    "                                \n",
    "                            if split_function == 'train_test_split':\n",
    "                                X_train, X_test, y_train, y_test = train_test_split(self.X, self.y, test_size=n/100, random_state=r)\n",
    "                                clf.fit(X_train, y_train)\n",
    "                                \n",
    "                                evaluator.load(X_test=X_test, y_true=y_test, clf=clf)\n",
    "                                scores = evaluator.get_metrics()\n",
    "                                now = datetime.now()\n",
    "                                dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "                                \n",
    "                                for score in scores:\n",
    "                                    #Save an entry or each metric\n",
    "                                    #self.results_df.loc[len(self.results_df)] = [self.xid,dt_string,self.X_name,'all',split_method, r, 0, models_to_run[index],p,score,scores[score]]                   \n",
    "                                    ins = students.insert(\n",
    "                                        values=dict(xid=self.xid,\n",
    "                                                    datetime=dt_string,\n",
    "                                                    dataset=self.X_name,\n",
    "                                                    subgroup='all',\n",
    "                                                    split_method=split_method,\n",
    "                                                    split_random_state=r,\n",
    "                                                    split_n=0,\n",
    "                                                    clf=models_to_run[index],\n",
    "                                                    params=p,\n",
    "                                                    metric=score,\n",
    "                                                    score=scores[score])\n",
    "                                    )\n",
    "                                    result = engine.execute(ins)\n",
    "                                    \n",
    "                                # Subgroup metrics\n",
    "                                for subgroup_var in config['subgroups'][self.X_name]:\n",
    "                                    for subgroup_value in list(X_test[subgroup_var].unique()):\n",
    "                                        X_test_temp = X_test[X_test[subgroup_var]==subgroup_value]\n",
    "                                        y_test_temp = y_test.loc[X_test[subgroup_var]==subgroup_value]\n",
    "\n",
    "                                        evaluator.load(X_test=X_test, y_true=y_test, clf=clf)\n",
    "                                        scores = evaluator.get_metrics()\n",
    "                                        now = datetime.now()\n",
    "                                        dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "                                        \n",
    "                                        subgroup_string = subgroup_var + '=' + str(subgroup_value)\n",
    "                                \n",
    "                                        for score in scores:\n",
    "                                            #Save an entry or each metric\n",
    "                                            #self.results_df.loc[len(self.results_df)] = [self.xid,dt_string,self.X_name,subgroup_string,split_method, r, 0, models_to_run[index],p,score,scores[score]]                                                        \n",
    "                                            ins = students.insert(\n",
    "                                                values=dict(xid=self.xid,\n",
    "                                                            datetime=dt_string,\n",
    "                                                            dataset=self.X_name,\n",
    "                                                            subgroup='all',\n",
    "                                                            split_method=split_method,\n",
    "                                                            split_random_state=r,\n",
    "                                                            split_n=0,\n",
    "                                                            clf=models_to_run[index],\n",
    "                                                            params=p,\n",
    "                                                            metric=score,\n",
    "                                                            score=scores[score])\n",
    "                                            )\n",
    "                                            result = engine.execute(ins)\n",
    "                                                                               \n",
    "                            elif split_function==\"StratifiedKFold\" or split_function==\"KFold\":\n",
    "                                \n",
    "                                kf = eval(f'{split_function}(n_splits={n}, random_state={r}, shuffle=True)')\n",
    "                                split_counter = 0\n",
    "                                for train_index, test_index in kf.split(self.X,self.y):\n",
    "                                    split_counter += 1\n",
    "                                    X_train, X_test = self.X.iloc[train_index], self.X.iloc[test_index]\n",
    "                                    y_train, y_test = self.y[train_index], self.y[test_index]\n",
    "                                    clf.fit(X_train, y_train)\n",
    "\n",
    "                                    # Here we run the model evalutor and save the stats\n",
    "                                    evaluator.load(X_test=X_test, y_true=y_test, clf=clf)\n",
    "                                    scores = evaluator.get_metrics()\n",
    "                                    now = datetime.now()\n",
    "                                    dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "\n",
    "                                    for score in scores:\n",
    "                                        #Save an entry or each metric\n",
    "                                        #self.results_df.loc[len(self.results_df)] = [self.xid,dt_string,self.X_name,'all',split_method, r, split_counter, models_to_run[index],p,score,scores[score]]                   \n",
    "                                        ins = students.insert(\n",
    "                                            values=dict(xid=self.xid,\n",
    "                                                        datetime=dt_string,\n",
    "                                                        dataset=self.X_name,\n",
    "                                                        subgroup='all',\n",
    "                                                        split_method=split_method,\n",
    "                                                        split_random_state=r,\n",
    "                                                        split_n=0,\n",
    "                                                        clf=models_to_run[index],\n",
    "                                                        params=p,\n",
    "                                                        metric=score,\n",
    "                                                        score=scores[score])\n",
    "                                        )\n",
    "                                        result = engine.execute(ins)\n",
    "                                        \n",
    "                                    # Subgroup metrics\n",
    "                                    for subgroup_var in config['subgroups'][self.X_name]:\n",
    "                                        for subgroup_value in list(X_test[subgroup_var].unique()):\n",
    "                                            X_test_temp = X_test[X_test[subgroup_var]==subgroup_value]\n",
    "                                            y_test_temp = y_test.loc[X_test[subgroup_var]==subgroup_value]\n",
    "\n",
    "                                            evaluator.load(X_test=X_test, y_true=y_test, clf=clf)\n",
    "                                            scores = evaluator.get_metrics()\n",
    "                                            now = datetime.now()\n",
    "                                            dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "\n",
    "                                            subgroup_string = subgroup_var + '=' + str(subgroup_value)\n",
    "\n",
    "                                            for score in scores:\n",
    "                                                #Save an entry or each metric\n",
    "                                                #self.results_df.loc[len(self.results_df)] = [self.xid,dt_string,self.X_name,subgroup_string,split_method, r, 0, models_to_run[index],p,score,scores[score]]\n",
    "                                                ins = students.insert(\n",
    "                                                    values=dict(xid=self.xid,\n",
    "                                                                datetime=dt_string,\n",
    "                                                                dataset=self.X_name,\n",
    "                                                                subgroup='all',\n",
    "                                                                split_method=split_method,\n",
    "                                                                split_random_state=r,\n",
    "                                                                split_n=0,\n",
    "                                                                clf=models_to_run[index],\n",
    "                                                                params=p,\n",
    "                                                                metric=score,\n",
    "                                                                score=scores[score])\n",
    "                                                )\n",
    "                                                result = engine.execute(ins)\n",
    "                                        \n",
    "                except IndexError as e:\n",
    "                    print('Error:',e)\n",
    "                    continue\n",
    "\n",
    "class ModelEvaluator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        clf=None,\n",
    "        X_test=None,\n",
    "        y_true=None,\n",
    "        metrics='test'\n",
    "    ):\n",
    "        self.clf = clf\n",
    "        self.X_test = X_test\n",
    "        self.y_true = y_true\n",
    "        self.set_metrics(metrics)\n",
    "    \n",
    "    def load(self,X_test,y_true,clf):\n",
    "        self.X_test = X_test\n",
    "        self.y_true = y_true\n",
    "        self.clf = clf\n",
    "        self.y_pred = self.clf.predict(self.X_test)\n",
    "        self.y_score = self.clf.predict_proba(self.X_test)[:,1]\n",
    "\n",
    "    # Check for presets\n",
    "    def set_metrics(self, metrics):\n",
    "        if metrics == 'small':\n",
    "            self.metrics = metric_list_small\n",
    "        elif metrics == 'test':\n",
    "            self.metrics = metric_list_test\n",
    "        else:\n",
    "            self.metrics = metrics\n",
    "            \n",
    "    def metric_at_k(self, metric, k):\n",
    "        y_pred = np.where(self.y_score > np.percentile(self.y_score,(100-k)), 1, 0)\n",
    "        s = eval(metric + '(self.y_true,y_pred)')\n",
    "        return s\n",
    "\n",
    "    def get_metrics(self):\n",
    "        results = {}\n",
    "        for metric in self.metrics:\n",
    "            if metric.find('@') > -1:\n",
    "                m = metric[0:metric.find('@')]\n",
    "                k = int(metric[metric.find('@')+1:])\n",
    "                s = self.metric_at_k(metric=m,k=k)\n",
    "            else:\n",
    "                s = eval(metric + '(self.y_true,self.y_pred)')\n",
    "            results[metric] = s\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read json\n",
    "with open('config.json') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "clfs = {}\n",
    "for clf in config['hyperparameters']:\n",
    "    clfs[clf] = clf_fcns[clf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fe156ed0a50d3a2dbeb3b48bb76a2b2f'"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xid = hashlib.md5((str(config)+datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")).encode('utf-8')).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(config['X'])):\n",
    "    X = pd.read_csv(config['X'][i],index_col=0)\n",
    "    y = pd.read_csv(config['y'][i],index_col=0,squeeze = True)\n",
    "    trainer = Trainer(xid=xid,\n",
    "                      X_name=config['X_name'][i],\n",
    "                      X=X,\n",
    "                      y=y,\n",
    "                      clfs=clfs,\n",
    "                      hyperparameters=config['hyperparameters'],\n",
    "                      split_methods=config['split_methods'],\n",
    "                      split_random_states=config['split_random_states'])\n",
    "    trainer.train_models()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
